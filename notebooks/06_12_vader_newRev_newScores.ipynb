{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06-12-vader-newRev-newScores.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRpHTdw9gL0q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "eed2ab7a-132e-4339-f71f-617f1f54b5e6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLp-MbwugMQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "43a83fc8-03a4-4496-b515-c16296107761"
      },
      "source": [
        "#import dask.dataframe as dd\n",
        "#!pip install fastparquet\n",
        "#from fastparquet import ParquetFile\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install vaderSentiment\n",
        "from nltk.corpus import wordnet \n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
        "\n",
        "nltk.download('wordnet')\n",
        "pd.options.display.max_rows = 999\n",
        "pd.options.display.max_columns = 999\n",
        "pd.set_option('display.max_colwidth', 1000)\n",
        "\n",
        "#imports to get our parquet files aand dask files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.6/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2.9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f55983f00437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvaderSentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvaderSentiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0memoji\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'emoji'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDOHZ320gMSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pandas dataframe\n",
        "def load_data(url):\n",
        "    if url.endswith(\".csv\") or \"csv\" in url:\n",
        "        df = pd.read_csv(url)\n",
        "    else:\n",
        "        df = None\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWvhM6c6gMW4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7c8321ec-9cea-4805-e216-d3cb5e15ca9d"
      },
      "source": [
        "rev_url = 'https://tally-ai-dspt3.s3.amazonaws.com/yelp-restaurants/rev_export.csv'\n",
        "\n",
        "rev = load_data(rev_url)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  if self.run_code(code, result):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSAs6JChrgXR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "ca3c1302-4213-4168-97ab-64ac39ed1836"
      },
      "source": [
        "rev.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>review_id</th>\n",
              "      <th>reviews_stars</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Oa-sapLub0wRAzxRKVmtDQ</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2017-02-23 03:10:38</td>\n",
              "      <td>Best thai food in Mesa, and you get so much fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>NawvdCkVIvhMO_eMVMdBhw</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2010-12-04 00:38:28</td>\n",
              "      <td>There are several contributing factors to the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>uBWpaM4_PQ1NMB3N9nYkJA</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2012-03-07 21:51:40</td>\n",
              "      <td>Moving to the Phoenix area was a shock to begi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>EIcE_Xw9G5bDAY-XQ0Vosg</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2011-10-06 00:29:51</td>\n",
              "      <td>Usually ok but today we went and ordered garde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>BXTxF_ijBmm5RPYELr-5RQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2013-11-20 03:08:02</td>\n",
              "      <td>I have never dined in. The restaurant itself i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Unnamed: 0  ...                                               text\n",
              "0          0  ...  Best thai food in Mesa, and you get so much fo...\n",
              "1          1  ...  There are several contributing factors to the ...\n",
              "2          2  ...  Moving to the Phoenix area was a shock to begi...\n",
              "3          3  ...  Usually ok but today we went and ordered garde...\n",
              "4          4  ...  I have never dined in. The restaurant itself i...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ok4Entmr5bt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "473f1d96-e19d-4a79-801b-c380559c901a"
      },
      "source": [
        "rev.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'review_id', 'reviews_stars', 'date', 'text'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ceyrqirkSr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "ace8584f-0a8b-4a7f-f1e9-e9db4bb86ce7"
      },
      "source": [
        "rev = rev.drop(columns=[\"Unnamed: 0\"])\n",
        "rev.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>reviews_stars</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Oa-sapLub0wRAzxRKVmtDQ</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2017-02-23 03:10:38</td>\n",
              "      <td>Best thai food in Mesa, and you get so much fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NawvdCkVIvhMO_eMVMdBhw</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2010-12-04 00:38:28</td>\n",
              "      <td>There are several contributing factors to the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>uBWpaM4_PQ1NMB3N9nYkJA</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2012-03-07 21:51:40</td>\n",
              "      <td>Moving to the Phoenix area was a shock to begi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EIcE_Xw9G5bDAY-XQ0Vosg</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2011-10-06 00:29:51</td>\n",
              "      <td>Usually ok but today we went and ordered garde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BXTxF_ijBmm5RPYELr-5RQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2013-11-20 03:08:02</td>\n",
              "      <td>I have never dined in. The restaurant itself i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                review_id  ...                                               text\n",
              "0  Oa-sapLub0wRAzxRKVmtDQ  ...  Best thai food in Mesa, and you get so much fo...\n",
              "1  NawvdCkVIvhMO_eMVMdBhw  ...  There are several contributing factors to the ...\n",
              "2  uBWpaM4_PQ1NMB3N9nYkJA  ...  Moving to the Phoenix area was a shock to begi...\n",
              "3  EIcE_Xw9G5bDAY-XQ0Vosg  ...  Usually ok but today we went and ordered garde...\n",
              "4  BXTxF_ijBmm5RPYELr-5RQ  ...  I have never dined in. The restaurant itself i...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGB2CSbmsU4O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "7da26857-dece-4ba8-8520-6a7e82a8873c"
      },
      "source": [
        "rev.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "review_id         object\n",
              "reviews_stars    float64\n",
              "date              object\n",
              "text              object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQaEDcrbGuV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#causes errors when running sentiment analysis\n",
        "rev['text'] = rev['text'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT_Pdc7blkGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rev['text'] = rev['text'].str.replace('\\d+', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xyqCatF56Bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rev['text'] = rev['text'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd2NRGYwnuNl",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis that worked on sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWxeozndgMa0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load VADER\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "#Add VADER metrics to dataframe\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y0n4Gx3DNA4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "f70763a7-b238-4428-e093-ccc9ba4d15a3"
      },
      "source": [
        "rev['compound'] = [analyzer.polarity_scores(v)['compound'] for v in rev['text']]\n",
        "rev['neg'] = [analyzer.polarity_scores(v)['neg'] for v in rev['text']]\n",
        "rev['neu'] = [analyzer.polarity_scores(v)['neu'] for v in rev['text']]\n",
        "rev['pos'] = [analyzer.polarity_scores(v)['pos'] for v in rev['text']]\n",
        "\n",
        "rev.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-57e57ced2c20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compound'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compound'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neu'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neu'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-57e57ced2c20>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compound'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compound'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neu'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neu'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/vaderSentiment/vaderSentiment.py\u001b[0m in \u001b[0;36mpolarity_scores\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0msentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment_valence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentitext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0msentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_but_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/vaderSentiment/vaderSentiment.py\u001b[0m in \u001b[0;36msentiment_valence\u001b[0;34m(self, valence, sentitext, item, i, sentiments)\u001b[0m\n\u001b[1;32m    312\u001b[0m                     \u001b[0mvalence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_negation_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                         \u001b[0mvalence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_special_idioms_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mvalence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_least_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGmvNOIDD17K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rev['text_length'] = rev['text'].str.len()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVExHctogMdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = ['review_id', 'business_id', 'compound']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvTZdkkBtrnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = rev[columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8RdqQPAqUqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizer(doc):\n",
        "     return [token for token in simple_preprocess(doc) \n",
        "             if token not in STOPWORDS]\n",
        "\n",
        "\n",
        "def related_to_food(doc):\n",
        "  word_similarity_list = []\n",
        "  for review_word in doc:\n",
        "    syns = wordnet.synsets(review_word) \n",
        "    if len(syns) > 0:\n",
        "      w1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
        "      w2 = wordnet.synset('food.n.01') \n",
        "      word_similarity_score = w1.wup_similarity(w2)\n",
        "      if word_similarity_score !=None and word_similarity_score > 0.5:\n",
        "        word_similarity_list.append(review_word)\n",
        "  return word_similarity_list\n",
        "\n",
        "\n",
        "def related_to_service(doc):\n",
        "  word_similarity_list = []\n",
        "  for review_word in doc:\n",
        "    syns = wordnet.synsets(review_word) \n",
        "    if len(syns) > 0:\n",
        "      w1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
        "      w2 = wordnet.synset('service.n.01') \n",
        "      word_similarity_score = w1.wup_similarity(w2)\n",
        "      if word_similarity_score !=None and word_similarity_score > 0.5:\n",
        "        word_similarity_list.append(review_word)\n",
        "  return word_similarity_list\n",
        "\n",
        "\n",
        "def related_to_speed(doc):\n",
        "  word_similarity_list = []\n",
        "  for review_word in doc:\n",
        "    syns = wordnet.synsets(review_word) \n",
        "    if len(syns) > 0:\n",
        "      w1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
        "      w2 = wordnet.synset('speed.n.01') \n",
        "      word_similarity_score = w1.wup_similarity(w2)\n",
        "      if word_similarity_score !=None and word_similarity_score > 0.5:\n",
        "        word_similarity_list.append(review_word)\n",
        "  return word_similarity_list\n",
        "\n",
        "def related_to_price(doc):\n",
        "  word_similarity_list = []\n",
        "  for review_word in doc:\n",
        "    syns = wordnet.synsets(review_word) \n",
        "    if len(syns) > 0:\n",
        "      w1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
        "      w2 = wordnet.synset('price.n.01') \n",
        "      word_similarity_score = w1.wup_similarity(w2)\n",
        "      if word_similarity_score !=None and word_similarity_score > 0.5:\n",
        "        word_similarity_list.append(review_word)\n",
        "  return word_similarity_list\n",
        "\n",
        "\n",
        "def related_to_ambience(doc):\n",
        "  word_similarity_list = []\n",
        "  for review_word in doc:\n",
        "    syns = wordnet.synsets(review_word) \n",
        "    if len(syns) > 0:\n",
        "      w1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
        "      w2 = wordnet.synset('ambience.n.01') \n",
        "      word_similarity_score = w1.wup_similarity(w2)\n",
        "      if word_similarity_score !=None and word_similarity_score > 0.5:\n",
        "        word_similarity_list.append(review_word)\n",
        "  return word_similarity_list\n",
        "\n",
        "\n",
        "def related_to_experience(doc):\n",
        "  word_similarity_list = []\n",
        "  for review_word in doc:\n",
        "    syns = wordnet.synsets(review_word) \n",
        "    if len(syns) > 0:\n",
        "      w1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
        "      w2 = wordnet.synset('experience.n.01') \n",
        "      word_similarity_score = w1.wup_similarity(w2)\n",
        "      if word_similarity_score !=None and word_similarity_score > 0.5:\n",
        "        word_similarity_list.append(review_word)\n",
        "  return word_similarity_list\n",
        "\n",
        "def extract_subject_related_words():\n",
        "  df['text'] = df['text'].apply(lambda x:\" \".join(re.findall(\"[a-zA-Z]+\", x)))\n",
        "  df['cleaned'] = df['text'].apply(tokenizer)\n",
        "  df['words_related_to_food'] = df['cleaned'].apply(related_to_food)\n",
        "  df['words_related_to_service'] = df['cleaned'].apply(related_to_service)\n",
        "  df['words_related_to_speed'] = df['cleaned'].apply(related_to_speed)\n",
        "  df['words_related_to_price'] = df['cleaned'].apply(related_to_price)\n",
        "  df['words_related_to_ambience'] = df['cleaned'].apply(related_to_ambience)\n",
        "  df['words_related_to_experience'] = df['cleaned'].apply(related_to_experience)\n",
        "\n",
        "extract_subject_related_words()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GobhZJZ07WVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentiment_score(sentence):\n",
        "  # Create a SentimentIntensityAnalyzer object. \n",
        "  sid_obj = SentimentIntensityAnalyzer()\n",
        "  # polarity_scores method of SentimentIntensityAnalyzer oject gives a sentiment dictionary. which contains pos, neg, neu, and compound scores. \n",
        "  sentiment_dict = sid_obj.polarity_scores(sentence)\n",
        "  return sentiment_dict\n",
        "\n",
        "def get_sentiment(review_list):\n",
        "  all_sentiments = []\n",
        "  compounds = []\n",
        "  if len(review_list) > 0:\n",
        "    for review in review_list:\n",
        "      score = sentiment_score(review)\n",
        "      all_sentiments.append(score)\n",
        "\n",
        "  if len(all_sentiments) > 0:\n",
        "    for sentiment_dict in all_sentiments:\n",
        "      compound = sentiment_dict['compound']\n",
        "      compounds.append(compound)\n",
        "\n",
        "  if len(compounds) > 0:\n",
        "    avg_sentiment = sum(compounds) / len(compounds)\n",
        "\t\n",
        "  else:\n",
        "    avg_sentiment = None\n",
        "    \n",
        "  return avg_sentiment\n",
        "\n",
        "def get_subject_review_scores(review_list):\n",
        "  if len(review_list) > 0:\n",
        "    sub_sentiment_score = round((get_sentiment(review_list))*150)\n",
        "    if sub_sentiment_score < 0:\n",
        "      sub_sentiment_score = 0\n",
        "    else:\n",
        "      sub_sentiment_score = 75 #leaving at 75 to give neutral score\n",
        "    return sub_sentiment_score\n",
        "\n",
        "def sentimental_analysis(df):\n",
        "  # Create a SentimentIntensityAnalyzer object. \n",
        "  sid_obj = SentimentIntensityAnalyzer()\n",
        "  # extract_subject_related_words\n",
        "  df['text'] = df['text'].apply(lambda x:\" \".join(re.findall(\"[a-zA-Z]+\", x)))\n",
        "  df['cleaned'] = df['text'].apply(tokenizer)\n",
        "\n",
        "  df['words_related_to_food'] = df['cleaned'].apply(related_to_food)\n",
        "  df['food_review_list'] = df[df['words_related_to_food'].map(len) > 1]['text'].tolist()\n",
        "  df['food_score'] = df['food_review_list'].apply(get_subject_review_scores)\n",
        "\n",
        "  df['words_related_to_service'] = df['cleaned'].apply(related_to_service)\n",
        "  df['service_review_list'] = df[df['words_related_to_service'].map(len) > 1]['text'].tolist()\n",
        "  df['service_score'] = df['service_review_list'].apply(get_subject_review_scores)\n",
        "\n",
        "  df['words_related_to_speed'] = df['cleaned'].apply(related_to_service)\n",
        "  df['speed_review_list'] = df[df['words_related_to_speed'].map(len) > 1]['text'].tolist()\n",
        "  df['speed_score'] = df['speed_review_list'].apply(get_subject_review_scores)\n",
        "\n",
        "  df['words_related_to_price'] = df['cleaned'].apply(related_to_service)\n",
        "  df['price_review_list'] = df[df['words_related_to_price'].map(len) > 1]['text'].tolist()\n",
        "  df['price_score'] = df['price_review_list'].apply(get_subject_review_scores)\n",
        "\n",
        "  df['words_related_to_ambience'] = df['cleaned'].apply(related_to_service)\n",
        "  df['ambience_review_list'] = df[df['words_related_to_ambience'].map(len) > 1]['text'].tolist()\n",
        "  df['ambience_score'] = df['ambience_review_list'].apply(get_subject_review_scores)\n",
        "\n",
        "  df['words_related_to_experience'] = df['cleaned'].apply(related_to_service)\n",
        "  df['experience_review_list'] = df[df['words_related_to_experience'].map(len) > 1]['text'].tolist()\n",
        "  df['experience_score'] = df['experience_review_list'].apply(get_subject_review_scores)\n",
        "  \n",
        "  return df\n",
        "\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swktlrk-bEWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukqcdG61a_t4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('sentiment.csv')\n",
        "!cp sentiment.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVtfG2vWgMhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def sentiment_score(sentence):\n",
        "#   # Create a SentimentIntensityAnalyzer object. \n",
        "#   sid_obj = SentimentIntensityAnalyzer()\n",
        "#   # polarity_scores method of SentimentIntensityAnalyzer oject gives a sentiment dictionary. which contains pos, neg, neu, and compound scores. \n",
        "#   sentiment_dict = sid_obj.polarity_scores(sentence)\n",
        "#   return sentiment_dict\n",
        "\n",
        "# def get_sentiment(review_list):\n",
        "#   all_sentiments = []\n",
        "#   compounds = []\n",
        "#   if len(review_list) > 0:\n",
        "#     for review in review_list:\n",
        "#       score = sentiment_score(review)\n",
        "#       all_sentiments.append(score)\n",
        "#   if len(all_sentiments) > 0:\n",
        "#     for sentiment_dict in all_sentiments:\n",
        "#       compound = sentiment_dict['compound']\n",
        "#       compounds.append(compound)\n",
        "# \tif len(compounds) > 0:\n",
        "# \t\tavg_sentiment = sum(compounds) / len(compounds)\n",
        "# \telse:\n",
        "# \t\tavg_sentiment = None\n",
        "# \treturn avg_sentiment\n",
        "\n",
        "# def get_subject_review_scores(review_list):\n",
        "#     if len(review_list) > 0:\n",
        "# \t    sub_sentiment_score = round((get_sentiment(review_list))*150)\n",
        "# \t\tif sub_sentiment_score < 0:\n",
        "# \t\t    sub_sentiment_score = 0\n",
        "# \t\telse: \n",
        "# \t\t\tsub_sentiment_score = 75 #leaving at 75 to give neutral score\n",
        "#     return sub_sentiment_score\n",
        "\n",
        "# def sentimental_analysis(df):\n",
        "#   # Create a SentimentIntensityAnalyzer object. \n",
        "#   sid_obj = SentimentIntensityAnalyzer()\n",
        "#   # extract_subject_related_words\n",
        "#   df['text'] = df['text'].apply(lambda x:\" \".join(re.findall(\"[a-zA-Z]+\", x)))\n",
        "#   df['cleaned'] = df['text'].apply(tokenizer)\n",
        "#   df['words_related_to_food'] = df['cleaned'].apply(related_to_food)\n",
        "#   df['food_review_list'] = df[df['words_related_to_food'].map(len) > 1]['text'].tolist()\n",
        "#   df['food_score'] = df['food_review_list'].apply(get_subject_review_scores)\n",
        "# \tdf['words_related_to_service'] = df['cleaned'].apply(related_to_service)\n",
        "#   df['service_review_list'] = df[df['words_related_to_service'].map(len) > 1]['text'].tolist()\n",
        "#   df['service_score'] = df['service_review_list'].apply(get_subject_review_scores)\n",
        "# \tdf['words_related_to_speed'] = df['cleaned'].apply(related_to_service)\n",
        "#   df['speed_review_list'] = df[df['words_related_to_speed'].map(len) > 1]['text'].tolist()\n",
        "#   df['speed_score'] = df['speed_review_list'].apply(get_subject_review_scores)\n",
        "# \tdf['words_related_to_price'] = df['cleaned'].apply(related_to_service)\n",
        "#   df['price_review_list'] = df[df['words_related_to_price'].map(len) > 1]['text'].tolist()\n",
        "#   df['price_score'] = df['price_review_list'].apply(get_subject_review_scores)\n",
        "# \tdf['words_related_to_ambience'] = df['cleaned'].apply(related_to_service)\n",
        "#   df['ambience_review_list'] = df[df['words_related_to_ambience'].map(len) > 1]['text'].tolist()\n",
        "#   df['ambience_score'] = df['ambience_review_list'].apply(get_subject_review_scores)\n",
        "# \tdf['words_related_to_experience'] = df['cleaned'].apply(related_to_service)\n",
        "#   df['experience_review_list'] = df[df['words_related_to_experience'].map(len) > 1]['text'].tolist()\n",
        "#   df['experience_score'] = df['experience_review_list'].apply(get_subject_review_scores)\n",
        "#   return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZB5cfU_gMVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzNFgShaRedW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('sentiment.csv')\n",
        "!cp df.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}