{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In order to use Gensim\n",
    "1. Activate Conda Python 3 `source activate python3`\n",
    "2. Install Gensim `conda install -c conda-forge gensim'\n",
    "3. Restart notebook kernal ensuring you are on conda_python3 environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In order to use Vader\n",
    "1. pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet \n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "\n",
    "# from tallylib.sql import getLatestReviews # for django app\n",
    "\n",
    "nltk.download('wordnet')\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import dask.dataframe as dd\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket='tally-ai-dspt3'\n",
    "folder = 'biz-rev-analytic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'rev.*.part'\n",
    "bucket='tally-ai-dspt3'\n",
    "folder = 'biz-rev-analytic'\n",
    "data_location = 's3://{}/{}/{}'.format(bucket, folder, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "s3://tally-ai-dspt3/biz-rev-analytic/rev.*.part resolved to no files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1fc9b8e82f9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/dask/dataframe/io/csv.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m                            \u001b[0menforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menforce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massume_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massume_missing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                            \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                            **kwargs)\n\u001b[0m\u001b[1;32m    427\u001b[0m     read.__doc__ = READ_DOC_TEMPLATE.format(reader=reader_name,\n\u001b[1;32m    428\u001b[0m                                             file_type=file_type)\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/dask/dataframe/io/csv.py\u001b[0m in \u001b[0;36mread_pandas\u001b[0;34m(reader, urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m                                   \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                                   \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                                   **(storage_options or {}))\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/dask/bytes/core.py\u001b[0m in \u001b[0;36mread_bytes\u001b[0;34m(urlpath, delimiter, not_zero, blocksize, sample, compression, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s resolved to no files\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0murlpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mblocksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: s3://tally-ai-dspt3/biz-rev-analytic/rev.*.part resolved to no files"
     ]
    }
   ],
   "source": [
    "df = dd.read_csv(data_location) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/My Drive/cafesFromDB.csv')\n",
    "#command ran in database\n",
    "# select * from tallyds.yelp_review where text like '%cafe%' order by business_id limit 500;\n",
    "\n",
    "df = df[df['business_id'] == '0d6kx6Jlocw77y1J9nbqMA']# just work with one popular cafe\n",
    "\n",
    "def tokenizer(doc):\n",
    "     return [token for token in simple_preprocess(doc) \n",
    "             if token not in STOPWORDS]\n",
    "\n",
    "\n",
    "def related_to_food(doc):\n",
    "  word_similarity_list = []\n",
    "  for review_word in doc:\n",
    "    syns = wordnet.synsets(review_word) \n",
    "    if len(syns) > 0:\n",
    "      w1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "      w2 = wordnet.synset('food.n.01') \n",
    "      word_similarity_score = w1.wup_similarity(w2)\n",
    "      if word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "        word_similarity_list.append(review_word)\n",
    "  return word_similarity_list\n",
    "\n",
    "\n",
    "def related_to_service(doc):\n",
    "  word_similarity_list = []\n",
    "  for review_word in doc:\n",
    "    syns = wordnet.synsets(review_word) \n",
    "    if len(syns) > 0:\n",
    "      w1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "      w2 = wordnet.synset('service.n.01') \n",
    "      word_similarity_score = w1.wup_similarity(w2)\n",
    "      if word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "        word_similarity_list.append(review_word)\n",
    "  return word_similarity_list\n",
    "\n",
    "\n",
    "def related_to_speed(doc):\n",
    "  word_similarity_list = []\n",
    "  for review_word in doc:\n",
    "    syns = wordnet.synsets(review_word) \n",
    "    if len(syns) > 0:\n",
    "      w1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "      w2 = wordnet.synset('speed.n.01') \n",
    "      word_similarity_score = w1.wup_similarity(w2)\n",
    "      if word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "        word_similarity_list.append(review_word)\n",
    "  return word_similarity_list\n",
    "\n",
    "\n",
    "def related_to_price(doc):\n",
    "  word_similarity_list = []\n",
    "  for review_word in doc:\n",
    "    syns = wordnet.synsets(review_word) \n",
    "    if len(syns) > 0:\n",
    "      w1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "      w2 = wordnet.synset('price.n.01') \n",
    "      word_similarity_score = w1.wup_similarity(w2)\n",
    "      if word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "        word_similarity_list.append(review_word)\n",
    "  return word_similarity_list\n",
    "\n",
    "\n",
    "def related_to_ambience(doc):\n",
    "  word_similarity_list = []\n",
    "  for review_word in doc:\n",
    "    syns = wordnet.synsets(review_word) \n",
    "    if len(syns) > 0:\n",
    "      w1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "      w2 = wordnet.synset('ambience.n.01') \n",
    "      word_similarity_score = w1.wup_similarity(w2)\n",
    "      if word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "        word_similarity_list.append(review_word)\n",
    "  return word_similarity_list\n",
    "\n",
    "\n",
    "def related_to_experience(doc):\n",
    "  word_similarity_list = []\n",
    "  for review_word in doc:\n",
    "    syns = wordnet.synsets(review_word) \n",
    "    if len(syns) > 0:\n",
    "      w1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "      w2 = wordnet.synset('experience.n.01') \n",
    "      word_similarity_score = w1.wup_similarity(w2)\n",
    "      if word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "        word_similarity_list.append(review_word)\n",
    "  return word_similarity_list\n",
    "\n",
    "def extract_subject_related_words():\n",
    "  df['text'] = df['text'].apply(lambda x:\" \".join(re.findall(\"[a-zA-Z]+\", x)))\n",
    "  df['cleaned'] = df['text'].apply(tokenizer)\n",
    "  df['words_related_to_food'] = df['cleaned'].apply(related_to_food)\n",
    "  df['words_related_to_service'] = df['cleaned'].apply(related_to_service)\n",
    "  df['words_related_to_speed'] = df['cleaned'].apply(related_to_speed)\n",
    "  df['words_related_to_price'] = df['cleaned'].apply(related_to_price)\n",
    "  df['words_related_to_ambience'] = df['cleaned'].apply(related_to_ambience)\n",
    "  df['words_related_to_experience'] = df['cleaned'].apply(related_to_experience)\n",
    "\n",
    "extract_subject_related_words()\n",
    "df.sample(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[(df['words_related_to_food'].map(len) > 1) | (df['words_related_to_service'].map(len) > 1) | (df['words_related_to_speed'].map(len) > 1) | (df['words_related_to_price'].map(len) > 1) | (df['words_related_to_ambience'].map(len) > 1) | (df['words_related_to_experience'].map(len) > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_reviews_containing_subject():\n",
    "  food_review_list = df[df['words_related_to_food'].map(len) > 1]['text'].tolist()\n",
    "  service_review_list = df[df['words_related_to_service'].map(len) > 1]['text'].tolist()\n",
    "  speed_review_list = df[df['words_related_to_speed'].map(len) > 1]['text'].tolist()\n",
    "  price_review_list = df[df['words_related_to_price'].map(len) > 1]['text'].tolist()\n",
    "  ambience_review_list = df[df['words_related_to_ambience'].map(len) > 1]['text'].tolist()\n",
    "  experience_review_list = df[df['words_related_to_experience'].map(len) > 1]['text'].tolist()\n",
    "  return food_review_list, service_review_list, speed_review_list, price_review_list, ambience_review_list, experience_review_list\n",
    "\n",
    "food_review_list, service_review_list, speed_review_list, price_review_list, ambience_review_list, experience_review_list = get_list_reviews_containing_subject()\n",
    "  # food_review_list = df[df['words_related_to_food'].map(len) > 1]['text'].tolist()\n",
    "  # service_review_list = df[df['words_related_to_service'].map(len) > 1]['text'].tolist()\n",
    "  # speed_review_list = df[df['words_related_to_speed'].map(len) > 1]['text'].tolist()\n",
    "  # price_review_list = df[df['words_related_to_price'].map(len) > 1]['text'].tolist()\n",
    "  # ambience_review_list = df[df['words_related_to_ambience'].map(len) > 1]['text'].tolist()\n",
    "  # experience_review_list = df[df['words_related_to_experience'].map(len) > 1]['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Sentiment Score (also very slow here) \n",
    "## Make EC2 instance and see what happens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(sentence):\n",
    "    # Create a SentimentIntensityAnalyzer object. \n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # polarity_scores method of SentimentIntensityAnalyzer oject gives a sentiment dictionary. which contains pos, neg, neu, and compound scores. \n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "\n",
    "    return sentiment_dict\n",
    "\n",
    "def get_sentiment(review_list):\n",
    "    all_sentiments = []\n",
    "    compounds = []\n",
    "\n",
    "    if len(review_list) > 0:\n",
    "        for review in review_list:\n",
    "            score = sentiment_score(review)\n",
    "            all_sentiments.append(score)\n",
    "\n",
    "    if len(all_sentiments) > 0:\n",
    "        for sentiment_dict in all_sentiments:\n",
    "            compound = sentiment_dict['compound']\n",
    "            compounds.append(compound)\n",
    "\n",
    "    if len(compounds) > 0:\n",
    "        avg_sentiment = sum(compounds) / len(compounds)\n",
    "    \n",
    "    else:\n",
    "        avg_sentiment = None\n",
    "\n",
    "    return avg_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores():\n",
    "  if len(food_review_list) > 0:\n",
    "    food_sentiment_score = round((get_sentiment(food_review_list))*150)\n",
    "  else: \n",
    "    food_sentiment_score = 75\n",
    "  if len(service_review_list) > 0:\n",
    "    service_sentiment_score = round((get_sentiment(service_review_list))*150)\n",
    "  else: \n",
    "    service_sentiment_score = 75\n",
    "  if len(speed_review_list) > 0:\n",
    "    speed_sentiment_score = round((get_sentiment(speed_review_list))*150)\n",
    "  else: \n",
    "    speed_sentiment_score = 75\n",
    "  if len(price_review_list) > 0:\n",
    "    price_sentiment_score = round((get_sentiment(price_review_list))*150)\n",
    "  else: \n",
    "    price_sentiment_score = 75\n",
    "  if len(ambience_review_list) > 0:\n",
    "    ambience_sentiment_score = round((get_sentiment(ambience_review_list))*150)\n",
    "  else: \n",
    "    ambience_sentiment_score = 75\n",
    "  if len(experience_review_list) > 0:\n",
    "    experience_sentiment_score = round((get_sentiment(experience_review_list))*150)\n",
    "  else: \n",
    "    experience_sentiment_score = 75\n",
    "  return food_sentiment_score, service_sentiment_score, speed_sentiment_score, price_sentiment_score, ambience_sentiment_score, experience_sentiment_score\n",
    "\n",
    "\n",
    "food_sentiment_score, service_sentiment_score, speed_sentiment_score, price_sentiment_score, ambience_sentiment_score, experience_sentiment_score = get_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = json.dumps([\n",
    "        { 'subject': 'Food', 'data1': food_sentiment_score, 'data2': 0, 'maxValue': 150 },\n",
    "        { 'subject': 'Service', 'data1': service_sentiment_score, 'data2': 0, 'maxValue': 150 },\n",
    "        { 'subject': 'Speed', 'data1': speed_sentiment_score, 'data2': 0, 'maxValue': 150 },\n",
    "        { 'subject': 'Price', 'data1': price_sentiment_score, 'data2': 0, 'maxValue': 150 },\n",
    "        { 'subject': 'Ambience', 'data1': ambience_sentiment_score, 'data2': 0, 'maxValue': 150},\n",
    "        { 'subject': 'Experience', 'data1': experience_sentiment_score, 'data2': 0, 'maxValue': 150}\n",
    "])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import re\n",
    "# import nltk\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from nltk.corpus import wordnet \n",
    "# from gensim.utils import simple_preprocess\n",
    "# from gensim.parsing.preprocessing import STOPWORDS\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "\n",
    "# from tallylib.sql import getLatestReviews\n",
    "\n",
    "\n",
    "# def yelpReviewSentiment(business_id):\n",
    "# \tdata = getLatestReviews(business_id, limit=200)\n",
    "# \tif len(data)==0:\n",
    "# \t\treturn {}\n",
    "# \tdf = pd.DataFrame(data, columns=['date', 'text', 'stars'])\n",
    "# \tdel data\n",
    "\n",
    "df = pd.read_csv('/content/drive/My Drive/cafesFromDB.csv')\n",
    "#command ran in database\n",
    "# select * from tallyds.yelp_review where text like '%cafe%' order by business_id limit 500;\n",
    "\n",
    "df = df[df['business_id'] == '0d6kx6Jlocw77y1J9nbqMA']# just work with one popular cafe\n",
    "\n",
    "\n",
    "def tokenizer(doc):\n",
    "\treturn [token for token in simple_preprocess(doc) \n",
    "\t\t\tif token not in STOPWORDS]\n",
    "\n",
    "\n",
    "def related_to_food(doc):\n",
    "\tword_similarity_list = []\n",
    "\tfor review_word in doc:\n",
    "\t\tsyns = wordnet.synsets(review_word) \n",
    "\t\tif len(syns) > 0:\n",
    "\t\t\tw1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "\t\t\tw2 = wordnet.synset('food.n.01') \n",
    "\t\t\tword_similarity_score = w1.wup_similarity(w2)\n",
    "\t\t\tif word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "\t\t\t\tword_similarity_list.append(review_word)\n",
    "\treturn word_similarity_list\n",
    "\n",
    "\n",
    "def related_to_service(doc):\n",
    "\tword_similarity_list = []\n",
    "\tfor review_word in doc:\n",
    "\t\tsyns = wordnet.synsets(review_word) \n",
    "\t\tif len(syns) > 0:\n",
    "\t\t\tw1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "\t\t\tw2 = wordnet.synset('service.n.01') \n",
    "\t\t\tword_similarity_score = w1.wup_similarity(w2)\n",
    "\t\t\tif word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "\t\t\t\tword_similarity_list.append(review_word)\n",
    "\treturn word_similarity_list\n",
    "\n",
    "def related_to_speed(doc):\n",
    "\tword_similarity_list = []\n",
    "\tfor review_word in doc:\n",
    "\t\tsyns = wordnet.synsets(review_word) \n",
    "\t\tif len(syns) > 0:\n",
    "\t\t\tw1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "\t\t\tw2 = wordnet.synset('wait.n.01') \n",
    "\t\t\tword_similarity_score = w1.wup_similarity(w2)\n",
    "\t\t\tif word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "\t\t\t\tword_similarity_list.append(review_word)\n",
    "\treturn word_similarity_list\n",
    "\n",
    "\n",
    "def related_to_price(doc):\n",
    "\tword_similarity_list = []\n",
    "\tfor review_word in doc:\n",
    "\t\tsyns = wordnet.synsets(review_word) \n",
    "\t\tif len(syns) > 0:\n",
    "\t\t\tw1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "\t\t\tw2 = wordnet.synset('price.n.01') \n",
    "\t\t\tword_similarity_score = w1.wup_similarity(w2)\n",
    "\t\t\tif word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "\t\t\t\tword_similarity_list.append(review_word)\n",
    "\treturn word_similarity_list\n",
    "\n",
    "\n",
    "def related_to_ambience(doc):\n",
    "\tword_similarity_list = []\n",
    "\tfor review_word in doc:\n",
    "\t\tsyns = wordnet.synsets(review_word) \n",
    "\t\tif len(syns) > 0:\n",
    "\t\t\tw1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "\t\t\tw2 = wordnet.synset('ambience.n.01') \n",
    "\t\t\tword_similarity_score = w1.wup_similarity(w2)\n",
    "\t\t\tif word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "\t\t\t\tword_similarity_list.append(review_word)\n",
    "\treturn word_similarity_list\n",
    "\n",
    "\n",
    "def related_to_experience(doc):\n",
    "\tword_similarity_list = []\n",
    "\tfor review_word in doc:\n",
    "\t\tsyns = wordnet.synsets(review_word) \n",
    "\t\tif len(syns) > 0:\n",
    "\t\t\tw1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "\t\t\tw2 = wordnet.synset('experience.n.01') \n",
    "\t\t\tword_similarity_score = w1.wup_similarity(w2)\n",
    "\t\t\tif word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "\t\t\t\tword_similarity_list.append(review_word)\n",
    "\treturn word_similarity_list\n",
    "\n",
    "def extract_subject_related_words():\n",
    "\tdf['text'] = df['text'].apply(lambda x:\" \".join(re.findall(\"[a-zA-Z]+\", x)))\n",
    "\tdf['cleaned'] = df['text'].apply(tokenizer)\n",
    "\tdf['words_related_to_food'] = df['cleaned'].apply(related_to_food)\n",
    "\tdf['words_related_to_service'] = df['cleaned'].apply(related_to_service)\n",
    "\tdf['words_related_to_speed'] = df['cleaned'].apply(related_to_speed)\n",
    "\tdf['words_related_to_price'] = df['cleaned'].apply(related_to_price)\n",
    "\tdf['words_related_to_ambience'] = df['cleaned'].apply(related_to_ambience)\n",
    "\tdf['words_related_to_experience'] = df['cleaned'].apply(related_to_experience)\n",
    "\n",
    "\n",
    "def get_list_reviews_containing_subject():\n",
    "\tfood_review_list = df[df['words_related_to_food'].map(len) > 1]['text'].tolist()\n",
    "\tservice_review_list = df[df['words_related_to_service'].map(len) > 1]['text'].tolist()\n",
    "\tspeed_review_list = df[df['words_related_to_speed'].map(len) > 1]['text'].tolist()\n",
    "\tprice_review_list = df[df['words_related_to_price'].map(len) > 1]['text'].tolist()\n",
    "\tambience_review_list = df[df['words_related_to_ambience'].map(len) > 1]['text'].tolist()\n",
    "\texperience_review_list = df[df['words_related_to_experience'].map(len) > 1]['text'].tolist()\n",
    "\treturn food_review_list, service_review_list, speed_review_list, price_review_list, ambience_review_list, experience_review_list\n",
    "\n",
    "\n",
    "\n",
    "def sentiment_score(sentence):\n",
    "\t# Create a SentimentIntensityAnalyzer object. \n",
    "\tsid_obj = SentimentIntensityAnalyzer()\n",
    "\t# polarity_scores method of SentimentIntensityAnalyzer oject gives a sentiment dictionary. which contains pos, neg, neu, and compound scores. \n",
    "\tsentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "\n",
    "\treturn sentiment_dict\n",
    "\n",
    "\n",
    "def get_sentiment(review_list):\n",
    "\tall_sentiments = []\n",
    "\tcompounds = []\n",
    "\n",
    "\tif len(review_list) > 0:\n",
    "\t\tfor review in review_list:\n",
    "\t\t\tscore = sentiment_score(review)\n",
    "\t\t\tall_sentiments.append(score)\n",
    "\n",
    "\tif len(all_sentiments) > 0:\n",
    "\t\tfor sentiment_dict in all_sentiments:\n",
    "\t\t\tcompound = sentiment_dict['compound']\n",
    "\t\t\tcompounds.append(compound)\n",
    "\n",
    "\tif len(compounds) > 0:\n",
    "\t\tavg_sentiment = sum(compounds) / len(compounds)\n",
    "\t\n",
    "\telse:\n",
    "\t\tavg_sentiment = None\n",
    "\n",
    "\treturn avg_sentiment\n",
    "\n",
    "\n",
    "# # tallylib/sentiment.py\n",
    "\n",
    "# import re\n",
    "# import nltk\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from nltk.corpus import wordnet \n",
    "# from gensim.utils import simple_preprocess\n",
    "# from gensim.parsing.preprocessing import STOPWORDS\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "\n",
    "# from tallylib.sql import getLatestReviews\n",
    "\n",
    "\n",
    "# def yelpReviewSentiment(business_id):\n",
    "# \tdata = getLatestReviews(business_id, limit=200)\n",
    "# \tif len(data)==0:\n",
    "# \t\treturn {}\n",
    "# \tdf = pd.DataFrame(data, columns=['date', 'text', 'stars'])\n",
    "# \tdel data\n",
    "\n",
    "df = pd.read_csv('/content/drive/My Drive/cafesFromDB.csv')\n",
    "#command ran in database\n",
    "# select * from tallyds.yelp_review where text like '%cafe%' order by business_id limit 500;\n",
    "\n",
    "df = df[df['business_id'] == '0d6kx6Jlocw77y1J9nbqMA']# just work with one popular cafe\n",
    "\n",
    "\n",
    "def tokenizer(doc):\n",
    "\treturn [token for token in simple_preprocess(doc) \n",
    "\t\t\tif token not in STOPWORDS]\n",
    "\n",
    "\n",
    "def related_to_food(doc):\n",
    "\tword_similarity_list = []\n",
    "\tfor review_word in doc:\n",
    "\t\tsyns = wordnet.synsets(review_word) \n",
    "\t\tif len(syns) > 0:\n",
    "\t\t\tw1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "\t\t\tw2 = wordnet.synset('food.n.01') \n",
    "\t\t\tword_similarity_score = w1.wup_similarity(w2)\n",
    "\t\t\tif word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "\t\t\t\tword_similarity_list.append(review_word)\n",
    "\treturn word_similarity_list\n",
    "\n",
    "\n",
    "def related_to_service(doc):\n",
    "\tword_similarity_list = []\n",
    "\tfor review_word in doc:\n",
    "\t\tsyns = wordnet.synsets(review_word) \n",
    "\t\tif len(syns) > 0:\n",
    "\t\t\tw1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "\t\t\tw2 = wordnet.synset('service.n.01') \n",
    "\t\t\tword_similarity_score = w1.wup_similarity(w2)\n",
    "\t\t\tif word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "\t\t\t\tword_similarity_list.append(review_word)\n",
    "\treturn word_similarity_list\n",
    "\n",
    "\n",
    "def related_to_speed(doc):\n",
    "\tword_similarity_list = []\n",
    "\tfor review_word in doc:\n",
    "\t\tsyns = wordnet.synsets(review_word) \n",
    "\t\tif len(syns) > 0:\n",
    "\t\t\tw1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "\t\t\tw2 = wordnet.synset('wait.n.01') \n",
    "\t\t\tword_similarity_score = w1.wup_similarity(w2)\n",
    "\t\t\tif word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "\t\t\t\tword_similarity_list.append(review_word)\n",
    "\treturn word_similarity_list\n",
    "\n",
    "\n",
    "def related_to_price(doc):\n",
    "\tword_similarity_list = []\n",
    "\tfor review_word in doc:\n",
    "\t\tsyns = wordnet.synsets(review_word) \n",
    "\t\tif len(syns) > 0:\n",
    "\t\t\tw1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "\t\t\tw2 = wordnet.synset('price.n.01') \n",
    "\t\t\tword_similarity_score = w1.wup_similarity(w2)\n",
    "\t\t\tif word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "\t\t\t\tword_similarity_list.append(review_word)\n",
    "\treturn word_similarity_list\n",
    "\n",
    "\n",
    "def related_to_ambience(doc):\n",
    "\tword_similarity_list = []\n",
    "\tfor review_word in doc:\n",
    "\t\tsyns = wordnet.synsets(review_word) \n",
    "\t\tif len(syns) > 0:\n",
    "\t\t\tw1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "\t\t\tw2 = wordnet.synset('ambience.n.01') \n",
    "\t\t\tword_similarity_score = w1.wup_similarity(w2)\n",
    "\t\t\tif word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "\t\t\t\tword_similarity_list.append(review_word)\n",
    "\treturn word_similarity_list\n",
    "\n",
    "\n",
    "def related_to_experience(doc):\n",
    "\tword_similarity_list = []\n",
    "\tfor review_word in doc:\n",
    "\t\tsyns = wordnet.synsets(review_word) \n",
    "\t\tif len(syns) > 0:\n",
    "\t\t\tw1 = wordnet.synset(syns[0].name()) # n here denotes the tag noun\n",
    "\t\t\tw2 = wordnet.synset('experience.n.01') \n",
    "\t\t\tword_similarity_score = w1.wup_similarity(w2)\n",
    "\t\t\tif word_similarity_score !=None and word_similarity_score > 0.5:\n",
    "\t\t\t\tword_similarity_list.append(review_word)\n",
    "\treturn word_similarity_list\n",
    "\n",
    "\n",
    "def extract_subject_related_words():\n",
    "\tdf['text'] = df['text'].apply(lambda x:\" \".join(re.findall(\"[a-zA-Z]+\", x)))\n",
    "\tdf['cleaned'] = df['text'].apply(tokenizer)\n",
    "\tdf['words_related_to_food'] = df['cleaned'].apply(related_to_food)\n",
    "\tdf['words_related_to_service'] = df['cleaned'].apply(related_to_service)\n",
    "\tdf['words_related_to_speed'] = df['cleaned'].apply(related_to_speed)\n",
    "\tdf['words_related_to_price'] = df['cleaned'].apply(related_to_price)\n",
    "\tdf['words_related_to_ambience'] = df['cleaned'].apply(related_to_ambience)\n",
    "\tdf['words_related_to_experience'] = df['cleaned'].apply(related_to_experience)\n",
    "\n",
    "\n",
    "def get_list_reviews_containing_subject():\n",
    "\tfood_review_list = df[df['words_related_to_food'].map(len) > 1]['text'].tolist()\n",
    "\tservice_review_list = df[df['words_related_to_service'].map(len) > 1]['text'].tolist()\n",
    "\tspeed_review_list = df[df['words_related_to_speed'].map(len) > 1]['text'].tolist()\n",
    "\tprice_review_list = df[df['words_related_to_price'].map(len) > 1]['text'].tolist()\n",
    "\tambience_review_list = df[df['words_related_to_ambience'].map(len) > 1]['text'].tolist()\n",
    "\texperience_review_list = df[df['words_related_to_experience'].map(len) > 1]['text'].tolist()\n",
    "\treturn food_review_list, service_review_list, speed_review_list, price_review_list, ambience_review_list, experience_review_list\n",
    "\n",
    "\n",
    "\n",
    "def sentiment_score(sentence):\n",
    "\t# Create a SentimentIntensityAnalyzer object. \n",
    "\tsid_obj = SentimentIntensityAnalyzer()\n",
    "\t# polarity_scores method of SentimentIntensityAnalyzer oject gives a sentiment dictionary. which contains pos, neg, neu, and compound scores. \n",
    "\tsentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "\n",
    "\treturn sentiment_dict\n",
    "\n",
    "\n",
    "def get_sentiment(review_list):\n",
    "\tall_sentiments = []\n",
    "\tcompounds = []\n",
    "\n",
    "\tif len(review_list) > 0:\n",
    "\t\tfor review in review_list:\n",
    "\t\t\tscore = sentiment_score(review)\n",
    "\t\t\tall_sentiments.append(score)\n",
    "\n",
    "\tif len(all_sentiments) > 0:\n",
    "\t\tfor sentiment_dict in all_sentiments:\n",
    "\t\t\tcompound = sentiment_dict['compound']\n",
    "\t\t\tcompounds.append(compound)\n",
    "\n",
    "\tif len(compounds) > 0:\n",
    "\t\tavg_sentiment = sum(compounds) / len(compounds)\n",
    "\t\n",
    "\telse:\n",
    "\t\tavg_sentiment = None\n",
    "\n",
    "\treturn avg_sentiment\n",
    "\n",
    "\n",
    "# def get_scores():\n",
    "# \tif len(food_review_list) > 0:\n",
    "# \t\tfood_sentiment_score = round((get_sentiment(food_review_list))*150)\n",
    "# \t\tif food_sentiment_score < 0:\n",
    "# \t\t\tfood_sentiment_score = 0\n",
    "# \telse: \n",
    "# \t\tfood_sentiment_score = 75\n",
    "# \tif len(service_review_list) > 0:\n",
    "# \t\tservice_sentiment_score = round((get_sentiment(service_review_list))*150)\n",
    "# \t\tif service_sentiment_score < 0:\n",
    "# \t\t\tservice_sentiment_score = 0\n",
    "# \telse: \n",
    "# \t\tservice_sentiment_score = 75\n",
    "# \tif len(speed_review_list) > 0:\n",
    "# \t\tspeed_sentiment_score = round((get_sentiment(speed_review_list))*150)\n",
    "# \t\tif speed_sentiment_score < 0:\n",
    "# \t\t\tspeed_sentiment_score = 0\n",
    "# \telse: \n",
    "# \t\tspeed_sentiment_score = 75\n",
    "# \tif len(price_review_list) > 0:\n",
    "# \t\tprice_sentiment_score = round((get_sentiment(price_review_list))*150)\n",
    "# \t\tif price_sentiment_score < 0:\n",
    "# \t\t\tprice_sentiment_score = 0\n",
    "# \telse: \n",
    "# \t\tprice_sentiment_score = 75\n",
    "# \tif len(ambience_review_list) > 0:\n",
    "# \t\tambience_sentiment_score = round((get_sentiment(ambience_review_list))*150)\n",
    "# \t\tif ambience_sentiment_score < 0:\n",
    "# \t\t\tambience_sentiment_score = 0\n",
    "# \telse: \n",
    "# \t\tambience_sentiment_score = 75\n",
    "# \tif len(experience_review_list) > 0:\n",
    "# \t\texperience_sentiment_score = round((get_sentiment(experience_review_list))*150)\n",
    "# \t\tif experience_sentiment_score < 0:\n",
    "# \t\t\texperience_sentiment_score = 0\n",
    "# \telse: \n",
    "# \t\texperience_sentiment_score = 75\n",
    "# \treturn food_sentiment_score, service_sentiment_score, speed_sentiment_score, price_sentiment_score, ambience_sentiment_score, experience_sentiment_score\n",
    "\n",
    "def get_scores():\n",
    "\tsubspecreviews = [food_review_list, service_review_list, speed_review_list, price_review_list, ambience_review_list, experience_review_list]\n",
    "\tscore_results = []\n",
    "\tfor subjectreviews in subspecreviews:\n",
    "\t\tif len(subjectreviews) > 0:\n",
    "\t\t\tsub_sentiment_score = round((get_sentiment(subjectreviews))*150)\n",
    "\t\t\tif sub_sentiment_score < 0:\n",
    "\t\t\t\tsub_sentiment_score = 0\n",
    "\t\telse: \n",
    "\t\t\tsub_sentiment_score = 75\n",
    "\t\t\tscore_results.append(sub_sentiment_score)\n",
    "\treturn score_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "extract_subject_related_words()\n",
    "food_review_list, service_review_list, speed_review_list, price_review_list, ambience_review_list, experience_review_list = get_list_reviews_containing_subject()\n",
    "subspecreviews = [food_review_list, service_review_list, speed_review_list, price_review_list, ambience_review_list, experience_review_list]\n",
    "result = get_scores()\n",
    "\t\n",
    "    \n",
    "    # result = json.dumps([\n",
    "# \t\t\t{ 'subject': 'Food', 'data1': food_sentiment_score, 'data2': 0, 'maxValue': 150 },\n",
    "# \t\t\t{ 'subject': 'Service', 'data1': service_sentiment_score, 'data2': 0, 'maxValue': 150 },\n",
    "# \t\t\t{ 'subject': 'Speed', 'data1': speed_sentiment_score, 'data2': 0, 'maxValue': 150 },\n",
    "# \t\t\t{ 'subject': 'Price', 'data1': price_sentiment_score, 'data2': 0, 'maxValue': 150 },\n",
    "# \t\t\t{ 'subject': 'Ambience', 'data1': ambience_sentiment_score, 'data2': 0, 'maxValue': 150},\n",
    "# \t\t\t{ 'subject': 'Experience', 'data1': experience_sentiment_score, 'data2': 0, 'maxValue': 150}\n",
    "# ])\n",
    "\n",
    "# del [food_sentiment_score, service_sentiment_score, speed_sentiment_score, price_sentiment_score, ambience_sentiment_score, experience_sentiment_score]\n",
    "\n",
    "# \treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "food_sentiment_score, service_sentiment_score, speed_sentiment_score, price_sentiment_score, ambience_sentiment_score, experience_sentiment_score = get_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Thoughts on condensing function\n",
    "\n",
    "# scoreslist = [food_sentiment_score, service_sentiment_score, speed_sentiment_score, price_sentiment_score, ambience_sentiment_score, experience_sentiment_score]\n",
    "# for score in scoreslist:\n",
    "#   viztype4 = {\n",
    "#       'subject': 'Food', 'data1': scoreslist[0], 'data2': 0, 'maxValue': 150 },\n",
    "#       'subject': 'Service', 'data1': scoreslist[1], 'data2': 0, 'maxValue': 150 },\n",
    "#       'subject': 'Speed', 'data1': scoreslist[2], 'data2': 0, 'maxValue': 150 },\n",
    "#       'subject': 'Price', 'data1': scoreslist[3], 'data2': 0, 'maxValue': 150 },     \n",
    "#       'subject': 'Ambience', 'data1': scoreslist[4], 'data2': 0, 'maxValue': 150 },\n",
    "#       'subject': 'Experience', 'data1': scoreslist[5], 'data2': 0, 'maxValue': 150 },   \n",
    "#   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Sample\n",
    "# [\n",
    "#         { subject: 'Subject 1', data1: 45, data2: 70, maxValue: 150 },\n",
    "#         { subject: 'Subject 2', data1: 75, data2: 95, maxValue: 150 },\n",
    "#         { subject: 'Subject 3', data1: 20, data2: 50, maxValue: 150 },\n",
    "#         { subject: 'Example Subject 4', data1: 65, data2: 85, maxValue: 150 },\n",
    "#         { subject: 'Food', data1: 35, data2: 45, maxValue: 150}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores():\n",
    "  subspecreviews = [food_review_list, service_review_list, speed_review_list, price_review_list, ambience_review_list, experience_review_list]\n",
    "  score_results = []\n",
    "  for subjectreviews in subspecreviews:\n",
    "    if len(subjectreviews) > 0:\n",
    "      sub_sentiment_score = round((get_sentiment(subjectreviews))*150)\n",
    "      if sub_sentiment_score < 0:\n",
    "        sub_sentiment_score = 0\n",
    "    else: \n",
    "      sub_sentiment_score = 75\n",
    "    score_results.append(sub_sentiment_score)\n",
    "  return score_results\n",
    "[food_sentiment_score, service_sentiment_score, speed_sentiment_score, price_sentiment_score, ambience_sentiment_score, experience_sentiment_score] = get_scores()\n",
    "food_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic = pd.DataFrame(df, columns = buisness_col)\n",
    "analytic.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
